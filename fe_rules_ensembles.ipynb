{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Feature engineering, rules-based models and ensembles "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b8e111e1e78dab2b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import shap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from wittgenstein import RIPPER\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV"
   ],
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-01T20:49:41.283291Z",
     "start_time": "2024-05-01T20:49:38.992925Z"
    }
   },
   "id": "initial_id",
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Import of clean data created in the first notebook"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d7c0bdfe805aa75a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('data/clean_data.csv')\n",
    "dataset = dataset.drop(dataset.columns[0], axis=1)\n",
    "test_dataset = pd.read_csv('data/test.csv').fillna(0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-01T20:49:41.882974Z",
     "start_time": "2024-05-01T20:49:41.284312Z"
    }
   },
   "id": "b94d620223124503",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algorithm\t\t Fold 1\tFold 2\tFold 3\tFold 4\tFold 5 \tAverage\n",
      "GNB\t 0.88\t0.90\t0.84\t0.89\t0.87 \t0.87\n",
      "LR\t 0.89\t0.90\t0.84\t0.89\t0.87 \t0.88\n",
      "RFC\t 0.86\t0.86\t0.82\t0.86\t0.85 \t0.85\n",
      "ETC\t 0.87\t0.87\t0.83\t0.86\t0.86 \t0.86\n",
      "XGB\t 0.88\t0.89\t0.84\t0.88\t0.87 \t0.87\n"
     ]
    }
   ],
   "source": [
    "def cross_validate_models(X, test_data, submission_file):\n",
    "    labels = X['Target']\n",
    "    cvm_data = X.drop(['Target', 'Date', 'Symbol', 'Id'], axis=1)\n",
    "    models = {\n",
    "        'GNB': GaussianNB(),\n",
    "        'LR': LogisticRegression(max_iter=1000),\n",
    "        'RFC': RandomForestClassifier(),\n",
    "        'ETC': ExtraTreesClassifier(),\n",
    "        'XGB': XGBClassifier()\n",
    "    }\n",
    "\n",
    "    folds = 5\n",
    "    kf = KFold(n_splits=folds)\n",
    "    best_model = None\n",
    "    best_score = -1\n",
    "    \n",
    "    print(\"Algorithm\\t\\t\", \"\\t\".join([f\"Fold {i+1}\" for i in range(folds)]), \"\\tAverage\")\n",
    "    for model_name, model in models.items():\n",
    "        scores = []\n",
    "        for train_index, test_index in kf.split(cvm_data):\n",
    "            X_train, X_test = cvm_data.iloc[train_index], cvm_data.iloc[test_index]\n",
    "            y_train, y_test = labels.iloc[train_index], labels.iloc[test_index]\n",
    "    \n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "            score = f1_score(y_test, y_pred)\n",
    "            scores.append(score)\n",
    "    \n",
    "        avg_score = np.mean(scores)\n",
    "        if avg_score > best_score:\n",
    "            best_score = avg_score\n",
    "            best_model = model\n",
    "\n",
    "        print(f\"{model_name}\\t\", \"\\t\".join([f\"{score:.2f}\" for score in scores]), f\"\\t{avg_score:.2f}\")\n",
    "\n",
    "    X_test_set = test_data.drop(['Symbol', 'Date', 'Id'], axis=1).fillna(0)\n",
    "    predictions = best_model.predict(X_test_set)\n",
    "    \n",
    "    submission_df = pd.DataFrame({\n",
    "        'Id': test_data['Id'],\n",
    "        'Predicted': predictions\n",
    "    })\n",
    "\n",
    "    submission_df.to_csv(f'{submission_file}.csv', index=False)\n",
    "    \n",
    "cross_val_data = dataset.copy()\n",
    "test_data = test_dataset.copy()\n",
    "cross_validate_models(cross_val_data, test_data, 'submission2')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-01T21:03:44.467785Z",
     "start_time": "2024-05-01T20:49:41.883995Z"
    }
   },
   "id": "9fdc050417390103",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Feature creation\n",
    "\n",
    "History related and domain specific features could prove to be useful as predictors so in the next chapter I will try to create new features that will enhance the accuracy of models the data is fed to."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "980fe328a0b653cb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### History related features\n",
    "\n",
    "1. **Lagged Returns (Lagged_Return_1, Lagged_Return_5):**\n",
    "   - These features capture the previous day's (Lagged_Return_1) and the return of the stock price five days ago (Lagged_Return_5).\n",
    "   - `shift(1)` and `shift(5)` shift the 'Close' price downward by one and five rows, respectively, effectively capturing the past returns.\n",
    "\n",
    "2. **Rolling Statistics (Rolling_Mean_5, Rolling_Std_5, Rolling_Min_5, Rolling_Max_5):**\n",
    "   - These features calculate rolling statistics over a 5-day window for each stock symbol.\n",
    "   - `x.rolling(window=5).mean()`, `x.rolling(window=5).std()`, `x.rolling(window=5).min()`, and `x.rolling(window=5).max()` compute the rolling mean, standard deviation, minimum, and maximum of the 'Close' price, respectively.\n",
    "\n",
    "3. **Price Oscillations (Price_Oscillation):**\n",
    "   - This feature computes the price oscillation, which is the ratio of the difference between the high and low prices to the closing price.\n",
    "   - It provides insights into intraday price volatility and trading range.\n",
    "\n",
    "4. **Volume Accumulation (Volume_Accumulation):**\n",
    "   - This feature calculates the cumulative volume traded for each stock symbol over time.\n",
    "   - `x.expanding().sum()` computes the expanding sum of the 'Volume' column, capturing the total volume accumulated until each point in time.\n",
    "\n",
    "5. **Price to Moving Average Ratio (Price_to_SMA_20_Ratio):**\n",
    "   - This feature calculates the ratio of the closing price to the 20-day Simple Moving Average (SMA).\n",
    "   - It indicates whether the current price is above or below the average price trend over the specified period.\n",
    "\n",
    "6. **Price Rate of Change (ROC):**\n",
    "   - This feature computes the rate of change of the closing price over a 5-day period.\n",
    "   - `x.pct_change(periods=5)` calculates the percentage change in the closing price relative to its value five days ago.\n",
    "\n",
    "7. **Volatility Measures (Volatility_10):**\n",
    "   - This feature computes the volatility of the stock price over a 10-day window.\n",
    "   - It measures the standard deviation of daily percentage changes in the closing price.\n",
    "\n",
    "8. **Price Crossings (Price_Above_SMA_50):**\n",
    "   - This binary feature indicates whether the closing price is above the 50-day Simple Moving Average (SMA_50).\n",
    "   - It can signal potential trend changes or breakout points.\n",
    "\n",
    "9. **Relative Price Strength (Relative_Price_Strength):**\n",
    "   - This feature calculates the ratio of the stock price to the average closing price of the market index on each date.\n",
    "   - It measures the stock's relative performance compared to the broader market.\n",
    "\n",
    "10. **Trading Volume Momentum (Volume_Momentum):**\n",
    "   - This feature computes the momentum of trading volume over a 5-day period.\n",
    "   - It measures the percentage change in trading volume relative to its value five days ago.\n",
    "\n",
    "These features capture various aspects of historical price movements, volatility, and trading activity, providing valuable information for predictive modeling and trading strategies."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "679f949c528b69eb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Domain specific features\n",
    "\n",
    "1. **Simple Moving Average (SMA_50):**\n",
    "   - This line calculates the 50-day Simple Moving Average (SMA) for each stock symbol in the dataset. \n",
    "   - `data.groupby('Symbol')['Close']` groups the data by the 'Symbol' column and selects the 'Close' price for each group.\n",
    "   - `.transform(lambda x: x.rolling(window=50).mean())` calculates the rolling mean (average) over a window of 50 periods for each group, which represents the 50-day SMA.\n",
    "\n",
    "2. **Exponential Moving Average (EMA_20):**\n",
    "   - This line calculates the 20-day Exponential Moving Average (EMA) for each stock symbol.\n",
    "   - Similar to SMA, it calculates the rolling mean, but EMA gives more weight to recent prices.\n",
    "   - `x.ewm(span=20, adjust=False).mean()` calculates the EMA with a span of 20 periods for each group.\n",
    "\n",
    "3. **Relative Strength Index (RSI):**\n",
    "   - RSI is a momentum oscillator that measures the speed and change of price movements.\n",
    "   - The `calculate_rsi()` function calculates the RSI for each stock symbol using the closing prices and a specified window (default is 14 periods).\n",
    "   - It calculates the average gain and loss over the specified window, computes the relative strength (RS), and then calculates the RSI using a formula.\n",
    "   - `data.groupby('Symbol').apply(calculate_rsi)` applies this function to each group of data grouped by 'Symbol'.\n",
    "\n",
    "4. **Bollinger Bands (SMA_20, std_20, upper_band, lower_band):**\n",
    "   - Bollinger Bands are volatility bands placed above and below a moving average.\n",
    "   - `SMA_20` calculates the 20-day SMA for each symbol.\n",
    "   - `std_20` calculates the standard deviation of closing prices over a 20-day window for each symbol.\n",
    "   - `upper_band` and `lower_band` are calculated by adding/subtracting two times the standard deviation from the SMA, providing a measure of volatility around the average.\n",
    "\n",
    "5. **Volume Weighted Average Price (VWAP):**\n",
    "   - VWAP is a trading benchmark used by traders that gives the average price a security has traded at throughout the day, based on both volume and price.\n",
    "   - This line calculates VWAP by summing up the product of volume and average price (computed as the average of high, low, and close prices) over time, and dividing by the cumulative volume.\n",
    "\n",
    "6. **Moving Average Convergence Divergence (MACD):**\n",
    "   - MACD is a trend-following momentum indicator that shows the relationship between two moving averages of a securityâ€™s price.\n",
    "   - `EMA_12` and `EMA_26` calculate the 12-day and 26-day EMA for each symbol, respectively.\n",
    "   - `MACD` is computed as the difference between the 12-day and 26-day EMAs.\n",
    "\n",
    "These features capture various aspects of price trends, volatility, and momentum, providing valuable insights for technical analysis in stock trading."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f44434cb6b353b01"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def calculate_rsi(close_prices, window=14):\n",
    "    delta = close_prices.diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=window, min_periods=1).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=window, min_periods=1).mean()\n",
    "\n",
    "    rs = gain / loss\n",
    "    rs[loss == 0] = 0\n",
    "\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi\n",
    "\n",
    "def transform_data(data):\n",
    "    data['SMA_50'] = data.groupby('Symbol')['Close'].transform(lambda x: x.rolling(50, min_periods=1).mean())\n",
    "    \n",
    "    data['Lagged_Return_1'] = data.groupby('Symbol')['Close'].shift(1)\n",
    "    data['Lagged_Return_1'] = data['Lagged_Return_1'].bfill()\n",
    "    \n",
    "    data['Lagged_Return_5'] = data.groupby('Symbol')['Close'].shift(5)\n",
    "    data['Lagged_Return_5'] = data['Lagged_Return_5'].bfill()\n",
    "    \n",
    "    data['Rolling_Mean_5'] = data.groupby('Symbol')['Close'].transform(lambda x: x.rolling(window=5, min_periods=1).mean())\n",
    "    \n",
    "    data['Rolling_Std_5'] = data.groupby('Symbol')['Close'].transform(lambda x: x.rolling(window=5, min_periods=1).std())\n",
    "    data['Rolling_Std_5'] = data['Rolling_Std_5'].fillna(0)\n",
    "    \n",
    "    data['Rolling_Min_5'] = data.groupby('Symbol')['Close'].transform(lambda x: x.rolling(window=5, min_periods=1).min())\n",
    "    \n",
    "    data['Rolling_Max_5'] = data.groupby('Symbol')['Close'].transform(lambda x: x.rolling(window=5, min_periods=1).max())\n",
    "    \n",
    "    data['Price_Oscillation'] = (data['High'] - data['Low']) / data['Close']\n",
    "    \n",
    "    data['Volume_Accumulation'] = data.groupby('Symbol')['Volume'].transform(lambda x: x.expanding().sum())\n",
    "    \n",
    "    data['SMA_20'] = data.groupby('Symbol')['Close'].transform(lambda x: x.rolling(window=20, min_periods=1).mean())\n",
    "    \n",
    "    data['Price_to_SMA_20_Ratio'] = data['Close'] / data['SMA_20']\n",
    "    \n",
    "    data['ROC'] = data.groupby('Symbol')['Close'].pct_change(periods=5)\n",
    "    data['ROC'] = data['ROC'].replace([np.inf, -np.inf], np.nan)\n",
    "    data['ROC'] = data['ROC'].fillna(0)\n",
    "    \n",
    "    data['Volatility_10'] = data.groupby('Symbol')['Close'].pct_change().rolling(window=10, min_periods=1).std().fillna(0)\n",
    "    \n",
    "    data['Price_Above_SMA_50'] = (data['Close'] > data['SMA_50']).astype(int)\n",
    "    \n",
    "    market_index_data = data.groupby('Date')['Close'].mean().rename('Market_Index').reset_index()\n",
    "    data = pd.merge(data, market_index_data, on='Date', how='left')\n",
    "    data['Relative_Price_Strength'] = data['Close'] / data['Market_Index']\n",
    "    \n",
    "    data['Volume_Momentum'] = data.groupby('Symbol')['Volume'].pct_change(periods=5)\n",
    "    data['Volume_Momentum'] = data['Volume_Momentum'].fillna(0)\n",
    "\n",
    "    data['RSI'] = data.groupby('Symbol')['Close'].transform(lambda x: calculate_rsi(x))\n",
    "    data['EMA_20'] = data.groupby('Symbol')['Close'].transform(lambda x: x.ewm(span=20, adjust=False).mean())\n",
    "    data['std_20'] = data.groupby('Symbol')['Close'].transform(lambda x: x.rolling(window=20, min_periods=1).std())\n",
    "    data['std_20'] = data['std_20'].fillna(0)\n",
    "    data['upper_band'] = data['SMA_20'] + (2 * data['std_20'])\n",
    "    data['lower_band'] = data['SMA_20'] - (2 * data['std_20'])\n",
    "    data['VWAP'] = (data['Volume'] * (data['High'] + data['Low'] + data['Close']) / 3).cumsum() / data['Volume'].cumsum()\n",
    "    data['EMA_12'] = data.groupby('Symbol')['Close'].transform(lambda x: x.ewm(span=12, adjust=False).mean())\n",
    "    data['EMA_26'] = data.groupby('Symbol')['Close'].transform(lambda x: x.ewm(span=26, adjust=False).mean())\n",
    "    data['MACD'] = data['EMA_12'] - data['EMA_26']\n",
    "    \n",
    "    return data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-01T21:03:44.483218Z",
     "start_time": "2024-05-01T21:03:44.469064Z"
    }
   },
   "id": "b04c687b3039129c",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cross validating the models on the created feature set\n",
    "\n",
    "To benchmark the quality of the newly created features multiple models will be cross-validated and their performance will then be compared."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6e0d10c34544a06e"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algorithm\t\t Fold 1\tFold 2\tFold 3\tFold 4\tFold 5 \tAverage\n",
      "GNB\t 0.88\t0.90\t0.84\t0.89\t0.86 \t0.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mpavlovic\\Documents\\fer\\stock-trading-predictions\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mpavlovic\\Documents\\fer\\stock-trading-predictions\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mpavlovic\\Documents\\fer\\stock-trading-predictions\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mpavlovic\\Documents\\fer\\stock-trading-predictions\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mpavlovic\\Documents\\fer\\stock-trading-predictions\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR\t 0.89\t0.90\t0.84\t0.89\t0.87 \t0.88\n"
     ]
    }
   ],
   "source": [
    "selected_features = ['Lagged_Return_1', 'Lagged_Return_5', 'Rolling_Mean_5', 'Rolling_Std_5', 'Rolling_Min_5', 'Rolling_Max_5', 'Price_Oscillation', 'SMA_20', 'EMA_20', 'Price_to_SMA_20_Ratio', 'ROC', 'Volatility_10', 'SMA_50', 'Price_Above_SMA_50', 'Relative_Price_Strength', 'Target', 'Date', 'Symbol', 'Id']\n",
    "\n",
    "transformed_data = dataset.copy()\n",
    "transformed_data = transform_data(transformed_data)[selected_features]\n",
    "test_data = test_dataset.copy()\n",
    "selected_features.remove('Target')\n",
    "transformed_test_data = transform_data(test_data)[selected_features]\n",
    "\n",
    "cross_validate_models(transformed_data.copy(), transformed_test_data.copy(), 'submission3')"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-05-01T21:03:44.487034Z"
    }
   },
   "id": "9d47598b9b357b9a",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Wrapper feature selection"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "af3b230334fa27e6"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def wrapper_feature_selection(data, features, target_features=12):\n",
    "    y = data['Target']\n",
    "    X = data.drop(['Target', 'Date', 'Symbol', 'Id'], axis=1)\n",
    "    model = GaussianNB()\n",
    "    remaining_features = features.copy()\n",
    "    \n",
    "    while len(remaining_features) > target_features:\n",
    "        f1_scores = []\n",
    "        for feature in remaining_features:\n",
    "            X_copy = X.drop(feature, axis=1)\n",
    "            model.fit(X_copy, y)\n",
    "            y_pred = model.predict(X_copy)\n",
    "            f1 = f1_score(y, y_pred)\n",
    "            f1_scores.append(f1)\n",
    "\n",
    "        best_idx = np.argmax(f1_scores)\n",
    "        remaining_features.pop(best_idx)\n",
    "        \n",
    "    return remaining_features"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "eca61e0a84dcaf8a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "features = ['Lagged_Return_1', 'Lagged_Return_5', 'Rolling_Mean_5', 'Rolling_Std_5', 'Rolling_Min_5', 'Rolling_Max_5', 'Price_Oscillation', 'SMA_20', 'EMA_20', 'Price_to_SMA_20_Ratio', 'ROC', 'Volatility_10', 'SMA_50', 'Price_Above_SMA_50', 'Relative_Price_Strength']\n",
    "best_features = wrapper_feature_selection(transformed_data, features)\n",
    "best_features.extend(['Target', 'Date', 'Symbol', 'Id'])\n",
    "selected_data = transformed_data[best_features]\n",
    "best_features.remove('Target')\n",
    "selected_test_data = transformed_test_data[best_features]\n",
    "cross_validate_models(selected_data, selected_test_data, 'submission4')"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "ad412762a5c9a549",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "X = selected_data.drop(columns=['Target', 'Date', 'Symbol', 'Id'])\n",
    "y = selected_data['Target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "best_ensemble_model = XGBClassifier()\n",
    "best_ensemble_model.fit(X_train, y_train)\n",
    "\n",
    "explainer = shap.TreeExplainer(best_ensemble_model)\n",
    "shap_values = explainer.shap_values(X_train)\n",
    "\n",
    "isolated_samples = selected_data.loc[selected_data.Id.isin(range(346055, 346065))]\n",
    "isolated_samples.reset_index(drop=True, inplace=True)\n",
    "for index, row in isolated_samples.iterrows():\n",
    "    row_df = pd.DataFrame([row[X_train.columns]])\n",
    "    sample_shap_values = explainer.shap_values(row_df)\n",
    "    shap.force_plot(explainer.expected_value, sample_shap_values, row_df, matplotlib=True)\n",
    "\n",
    "X = isolated_samples.drop(columns=['Target', 'Date', 'Symbol', 'Id'])\n",
    "y = np.array(isolated_samples['Target'])\n",
    "\n",
    "y_pred = best_ensemble_model.predict(X)\n",
    "print(f'y:{y}\\ny_predicted:{y_pred}')"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "3f58517ebec9cc3d",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ripper algorithm\n",
    "Next part of the notebook performs the Ripper algorithm on AAPL stocks"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2ad7444ee988b1a8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Grid search of prune size, dl_allowance and k hyperparameters"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bdefa717ad938f15"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "feature_data_aapl = selected_data[selected_data['Symbol'] == 'AAPL']\n",
    "\n",
    "X = feature_data_aapl.drop(columns=['Target', 'Date', 'Symbol', 'Id'])\n",
    "y = feature_data_aapl['Target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "def ripper_grid_search(max_rules, max_cond):\n",
    "    scorer = make_scorer(f1_score)\n",
    "    param_grid = {\n",
    "        'prune_size': [0.1, 0.2, 0.3],\n",
    "        'dl_allowance': [0.1, 0.2, 0.3],\n",
    "        'k': [2, 3, 4]\n",
    "    }\n",
    "    ripper = RIPPER(max_rules=max_rules, max_rule_conds=max_cond)\n",
    "    grid_search = GridSearchCV(ripper, param_grid, scoring=scorer)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_params = grid_search.best_params_\n",
    "    print(\"Best Parameters:\", best_params)\n",
    "    ripper_best = RIPPER(**best_params)\n",
    "    ripper_best.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred_best = ripper_best.predict(X_test)\n",
    "    f1_best = f1_score(y_test, y_pred_best)\n",
    "    acc_best = accuracy_score(y_test, y_pred_best)\n",
    "    print(\"Best F1 Score:\", f1_best)\n",
    "    print(\"Ripper accuracy:\", acc_best)\n",
    "    print(\"Decision Rules:\")\n",
    "    for rule in ripper_best.ruleset_:\n",
    "        print(rule)\n",
    "        \n",
    "ripper_grid_search(None, None)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "825d43984718bf95",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Maximum rule and conditions limitation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3ccf9903872afce"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "ripper_grid_search(3, 2)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "a18832b02a065947",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
